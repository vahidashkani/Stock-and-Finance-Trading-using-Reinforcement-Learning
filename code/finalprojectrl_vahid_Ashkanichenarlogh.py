# -*- coding: utf-8 -*-
"""FinalProjectRL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OsbsWqmgYzB3f0Jh9XsBckU9toFnSaZo

#################![western-university-vector-logo_0.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAATsAAACvCAYAAACCXHkmAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAAEnQAABJ0Ad5mH3gAAEAfSURBVHhe7Z0FgFPXEoYHKO5SoLi7Oyzu7lasaPFiBYq7FodCcXd3t+LursXdXd/5J/ey2bs3ulngNfO95pHcZCNX/jMzZ2ZOkM8KEgRB+I8TVPtXEAThP42InSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXoGInSAIXkGQzwrtviAIgikvn72hl8/f0P1bz+jJgxf06N4Levb4Jb15/Z4e3H5GQYIEUY9f07s375WqaH+koxRGPU1RoocnyE3EKGHphxDB6MefIlCEKGEoUtRwFCVGeLU9DIWNEIpChPxB+0PPImInCAIDMbty+i6dP36T/j13j65fekA31O3tmw/0+sVbev3qHQULFoSCBg1KQYLi3yAscrgPMcN9f0JnxedPFqn5hH/Vf58+feL72P7x4ycK9kNQCh0mBIUOG5Ii/RiO4iSOSgmSx6BEqWJSsvSxKUaciPzZ7mJT7O7dfEoP7jxTP87qze38EMbwTh/ef6REqWPyD7AGO/Xq2Xv0Q/Bg2hYNF98fvH/3gdJki689ss+5ozfVW3xWH2P4IP2h1fvjQISLGJriJo6mbRGE/xZnDl+ng1sv0pFdl+j6hfvqmn/CgoXrEsIT7Af1rxI3KFlQTchY0GwAKdGft6iK72Ng/bwZeB5/B/HD9ffxoxLBDx/V7ZO6zj8qqy8kRYsZkVJnjUfpcyakLAWSUvTYEbW/doxNsRvSZglNHLGewlBIfgwVDxcxlOmXxRt8Usr84ukbywaNJ/SSVu7vTmmy+hWj3evOUq0SQyiiendrwoYPRcGCq1FDe6yD98cOwPsbv+5Dek4XXk+gkKGCa1tskzNMO3r1+i0FtQpVhgwdnG94f5jqPOoo3tF7yl0oDY3f1JwfC8L/Oy+evqa1cw7R9hUnlNDdYGMELmNw5VJC2IIqYbMnRvbAdQlBwntZrIfP/P4QTrwnri8YJsHV57nzGboQQmfwvvist8plhtilzZGAClVIT3nLpNFebY5NsXv68CU9UbegyrKD0L1++ZYaFx5L4ZS6Gr8sPjxWgig0bGlDevzgpbaV6CO2J4zqzwd/q/z829ce8Q7WiRApNLUuO5Hu337qz+KDysN8nb6nDT1/8pp/tM4HtQMTpoypPbLP9Yv3LXe07x9KidymRUdpysCN/Jlj1zfj76qb1qHCBFc7MxK/VhD+n5k8YCON67aaIkULSyGUYYDzHde1q0Au9Osf13GIUBbxgj7kLpmadq8/o11Dnyhr/mS0d9M5Cq4+640SpmJVM9JGdb3BMIFm4POhL+6C7/JJWX/v33/g7/L88WtadLozJUgWXXuFX2x+UsSoYSm++iO4cbGVYCVJE4sy+iSgd28/aK/wBa7ulTN3KUy4kPx6/ZYgRQzTYCMsKfji1q/F51VunIteqZ1mBEL36N5zevboFcVJ5Ps3uDkrdCBukh8tN+1vf4wVUZnwl3mHpcuegBKq74vfiufiJf1RhE74z/D29TueBEBoBtekO0IHgcK1ApGBuJSuk1WJ3DveHi1mBOo0toqvd/Q5CP3avTi9evGWnj56SX8uqMev0V1SXLcRlfDCUsP7vVHfT/eqnAUiC3c7VOgQPOmByQ2eILGBS7JauUluDlT6Q+03BBjXLziibXCPkmrnfXxv+fFGQocNQYvG79YeeY5DWy+qg/WByjfMqW0RhP8emERwFVyH+rX44cNHNgCSZ4jF4obH8ZXBUr5+Dva2UmaORz8oTy11lnj06vkbSpcrIcWMF5nevHpHEZQQpcuZkEKpaxji9uzJK+rydzUWQrw9/r7sL9n5GtcFz0wDHKJ+oj0X2SWxy1EkhbLeQvEXtgYfECpMCFo7+6C2xT0wcZC9UHJT6zFEyOD0z6qT2iPPcHzvFXqvDlqY8KEpZ9EU2lYBTOq/gTpUnao9ErwNi9gEYSsMAgSBS5I2FtXtWISePHipidQratanFM/SQmhA35m1OQUlW4Gk/BgWXLJ0sfh+9sLJ6YWy/NJmi08x40ZiaxPhsv6z61CxnzPTvRuYILFYkLAA3RI8O7jsMBeslJ4Dg0bglx/fd1V75D6Vm/iwaWz8oQiePrzzjM4du6ltCTg7V59WR5Uod8mU2hZB5+DWC5w/JXgXuO4Qrwawyhp2LcaWGi7HR3efc+ZD/U4QvBfqeYsO/NKuIF07f4/vR1WuaunaWSlNdsukZNb8SSilsvYAwmLhI4Wm9iMr8eN3bz5QloJJ2Yjau/EsBVMa8l4ZOj/GjsgzrRA9T+Ky2FVrmptePof5aVBdpciYnt64MGCubLZCyZSlFfLLDteB9Yj8m2WT92hbAg6CpbBSy9TJrm0RdM4evsGxVcF7YKHDZadcXsvl/ZkNjBl727EFdu7oDWykBp2LUq3WBTgPDzTvV9pPbL58g5wckwcQLcT2dOD2IjYOnj56RY26Fuf74SOGYqGDuz11R2vav/kcx+NgWFncXYPeuIHLYhdX+e1xEkZlM9MaFiPlynoirlagbDpT6zFkqB/on5WntEcBAzmEd6495gzudDkTaFsFcPHUbZ7hshf/EP57QE/goTXvU5JFDjO26+cd5kyLoYsb0smT/9Jd5WqC1n+WY0tN58+F9bV7xNYfhArkLZ2GYmgTfYjrQyh1fIqlpJSZ4vB9hMHg0S08/gc9uv+cZ1bhDmfKk5jaDi3PfxtQwXNZ7EDZetm5TMQIcmiO7bqsLpSAmZ9Vm+dhE9n44zBN/eLZa2XyntO2uM/KGfv5wPqUSKVtEXQWjN3JaTmCd4DrDGKCse3R/RdUtGomjqO9fPZWWW/32ZPzKZGSeoyuwalaOjVb5dPuEYUOZ8nHBbrQGUHWBkRNZ9iyRto9opP7r9Gcw+2VVxeKNi44Ss/Vdd6wWzHqN6sOzRyyBUamIgh/T3dxS+wqNclNr575T/AF8Mnnj/lHe+Qe8ZP9yKau0XoESG9ZOG6H9sh9Ns4/Qh8/faKyv2TTtggA+VGrpu9TLqzfqhfhvwmuYUz+Rf4xHKeNQPDWzT1E+cqmpW0PB1DB8unUY8vEY/UWealsXd+QjzEfNiDUbleAkqT+ie+vUobIr8q9rdUqP109d5euX3zAqTLPH7/kdDB3Bc8tsQupLDhMLWOmxgjiPKtnHdAeuQ/y6sxygYKHUNbjnqumbq6zPFZm8pWzd9ntRva14AtmYJGzJB7sfx/dWHmsrLku46oqMcvHkxLD2y/j7WDA3LrKjY2qPbIYM4HBjz9pZV/qO2XJn5RnecHRnZd5NhjWZd8ZdShz3sSmuuMMbokdqN4Crqb/WVOYsNcu3KdbVx9qW1xn36Zzyl9/ZZobBAH8rKyPNbPcT3NZOf0Aj0r5yqXVtghg6sBNdGDrBc6KF/7b4LrFhACEBPltbStMooZdi9LCE524wqFH/TnaK+mrpmVBTVoNKmt5oFg+dS9lUgK38VYfypA7Ic0ZuY1L0mDsuDpb67bY5S+Xjv81aB0HtVHjuniC+7Omy6fuY98eP8bMVQ4VNiQtneT++29bfoLft1jVTNoWYVyPNTR10EaKFBVW3X/XrDPmiHorSPsoV9+SSI/GHKhOWjhuJ1cQrbjYneImjsqv+dpYn3vovlKxkQ+NWtWY4/Udq09TBpCSLCUJeUqn5k4orghegFo89W40l3avO+Mn6AgQa4OFt+xcV22LaxT5qSsFVX//a7fiNK77ai5xsQZf+enDVzT/aEeKEde1ki5YjCXj92SrcceTQdpW5/n3/D3av+U8XTxxi/PQHt97QUHUgUCSJErR0mSLR7mKp3KrHMcWu9aept0bztKlE7fZvcfvx1Q/yt1SZI5LmXInolRZ4vF+wkg9ouNyqqBO5IQpLVP89ji66zKN6LCcrXG4KPrJhqLtRKl+otGrG/NjT4HZvP3Kcr946g7dvPKAc7dQIx01Zni+0PA7chVLydaGJ8F+6Vp7JtXtUIgvEiMbFhzhtKmblx7whYXyxVzFU1LN1vm1V7jGyX1XadPiY9R6cDlti1+O77lCO9VxPbnvX06teP8WF+1nTrtKmjYWF7Uj/8xTjOu5hpZO3MMxbwCvrMwv2al531I0b8w/tH3FSTr8z0Xa8WwwhQjxfVj2GJj02tmD2y7QLwWGU9t+5XlG99LJ29Sq7AR+Xr/WHj94QZO2/cb7z4wAid2pA9eoefGxfGIYrQFclH+tbepyTGzH6lPUucZ0PiEn/9OKSiXoycKp/2gdjEZVmuTm+jtXwIH9q+tqKlghHfWaWlPb6pipAzfSiun7Ocv7A8Q8WBCK9lMEPnmQUY6SF+QaIqYIUAnSol8pFkB3mTt6O7uWb16+4xlutLhJli42CwFE4srZe/T61VsKHlx9pvpsCCBGOlSg9JtZmwpVyqC9ky/IlbqgRBP/7lUCivSb8JHDsGtgfQwRBI6gtheulN505h21kUWrZaRUmS0Jo45YMW0fzf9rB109e/eLdaXXaqIE8SGLuOU34HmkLzTqXowy+iTm19ritDoHNyiRMssJxO/BhAsuDMR50TVn4Yk/uPZZB22O4MLdvf5EfZdQWkwKeWafWRDg0s093IEL6AEE8eyRG6bBeZyjOC5nDl2js0dvUrm62bksypolSnD+VsKD16GBJc5tlFnh++O343M/vP/En41GFJ3GVqX8ZQMebtHFDucOGyPquz5T5+zsA79zagmAWBz55xIVrJieH39P/FpwNA8cGAxBweidlQcZkgdKvVFIoIodKJe0NyH/10/fOwUKe3MUTkE9p9TQtjjHb6XHs7XRckAZFjO0mtq48Ki/kZ5nZNQHr7zcQ9viHPXyjKCLx29xXlAOJ2IRO9ecph71ZuHK4ZPx1fO31FmdgMWq+3WBcXLOHbWdJvXfSFGjh+MgKrrGVGiQgzqMqqy9ynnq+YxQYnbHUtysRv3BC+pRRmXBGTl96DqN7rSCB55I0cLha3K+UoWGOalpr5Laq3xpXW4iJ2xCpJEqxN0v8Ef4z0rsAI7hneuPtUd+eU6vafD4+lTp11zaFnMun77DxxQzfUhPwKDQsEtRqtWmAF/o1iyfspdG/rGCX4ekciSyZi2QjEas8E1RMLJo3C7q2GwqhSdf6z9okKA8EAH8ImTm/xA8qPrsNzRjb9svYgevpFmJsZQkdSzqMflnSps9AU0bvJkHGRSXY3cgNhQufChaeLIT/w0G4iVzd1Mo8j0fI0QK88W7gZWBfYrBqUC5tPTHmCq8HZfZLzmG0eH9lyljtkR8/iRKGZPF9d6tp7R+7iHaowYfdPPVY9U4xx+o51oOKEs1rNI83IHFbsIeFnQ06EBcHN1PIBSz9v/uZwD43pk2eBNNG7SJDQscH/weVHQgSTlQxQ6j1BK1E2F+W4OdiFy5DTf7aFucI2/kjnzRrb3WW12QIdi9+jnzYN/ZGg18bVwMY5T16GzzTry+TJI+fOJvuddf22qbgS0X0UplkURVFgj2Ely7VUpc7U25n1Hi06TIGBYeAGsC5S/zlHXgLHV9htOtq49Y4DFTtlV9V1yw9kAThn6/zuMOr7DukmeIQ8OXNdSetc+gFoto64rjfIHr4LcmSRObRq60LTSOWDJxNw3+bTFFixWBR17s/yk7W1Fi5R7bAl1xyydTxwhWjxpAIdxo4LhIiY2+T53BJ0J7P+cMzhckqepid/nUbSqfph8VrZiRhixuwK9Bm/FCMbry7KOu+/g7WAxzDrSnODYEYXj75bR+3iE/4RwMfvk1sXun9iVCMz8ocR+x/Feb3s4J5fq2LPm3ZTZcEzx8/n0leFN3taHkJu63s+iWHa6tcvWy82zn4gm7afXMA7Rj5ylaf6o3dwT+3kFsMVvUtpQ5axIuSytfPyf9UX0qh0XQr8+e2PkdWt0ApSFQV6Nm4uTGbM/2lc4X72+Yf5jfJ506GSB0AJ0W4iWN7i8QiYOGkwsXlLNsUBbi58+fuCDZEd3rzuZ8I1gIOPEQ6xu/qYVdoQMpM8elBp2LaTlLloanj5WLVivrEO0V9pk5bCtdPXePhQ4XTJWmuR0KHUCvsOErfqUnShxhMZ068K/2jGNspfFA8NwFiclD2y7lmCrcNAhdj0k17AodQJUMF5OrkxrnAvYDXLzqGQd7ZHJBj+80LvwXZcmd9IvQATSVtcQttQ0M2lMEoft26oRhAZvBFrOiYop+bLFtuNXXblgHlmWjbsU5RKODcwgiP+J333SQgIB9OX/sDhrTdRVb5VN2tKITL8fYtOC/JxB3RTHAwSfDabayRn9umY+aFv2LvRsMJI4IsNjFjBuZEqeOaTorAndkyfhd2iPHLFYjD2ZajO2WkPhrFjdCPAUurrNsZDElKlkzi7bFnAXqZNi67DgvAIKTDfEpuCTxk5s3BTRS5/eChMYFODi6KN/69xGN+mOl9grbLFSfHT6iZaIArnDW/JbuEc6AHKSGXSxCi1tAss0DAoLJo5Q7ygOF+h04N+ImiUYFK1pm8B2ROV8Sfr2eVA53BccNAemAgrDA7BHb2FobvaaJttXCT/GjmHTc+cztjBKkcO7YW4NY6IyhW3hSZtGJTv5CPWYgpQvnjbXxgEqfc8dvcseQgIDQAIwIhGPmj9lBo7usUhbuHfaekKx7+uC17/p2/ugNnryC14M8WVjB+BeWMIwrzluxQ4DdWLB00m4ao3Zc+Eh+26xj5z5XpiWmssOpk8wROcK0o9BhQtLWB35dTFgY+aL8wbOPuHisQWPAHpNrOAziPlWWWfmkcGGD0cbbfbWt/sHsaqnEvXjlI3wWdg/HAra25BY3zvJ7pcl8gBAXAXifh3ef06pLPdRIbQl2G8G6HHVzDeOmhvhsWJN9ptUin5KulbRVSz+I8xzHbWzulIvfu+Fc2r3hjD83Nn6yGDR2fVNti/OUiGdx9fWYHOJ0v4+oSCV+zsyPnWFyvw08oaGHR7D/4NKjjAkdcR1h5sYipLD4VCcqrr5f/T+KmE5uVUzVj+syYVHiwoCFiZnRPtNrWV5gwoAWC2n78hN+3Fjsv4y5E9O2FSeo46hKPPPpLPXzjOT1IKy9CIjzSGW5Z/DxH7d1Brix80f/w7FC9KVsUniM2ieWGc//VzAoJE4Ti6Yq67RM4t6sBZjUDDQ3FpSslRX2Nn+4NXD/cNEi+OoI5M1hBM9eOJm2xRdsx0E2y5xGFQQC247YvuwEWxgZTIL81vRoMOeLRQfwmyIr8XFF6EBGQ6Y33g+zR2O7r9a2+OfSyVuccqN/NmZID/1zke+7QuMexdkSPn/Uc+2wnOWvrqv5QreefEDcrXBl/zPD9sBxQjMCHewTuJgTeq3TtrgOmkfOGraV79uaxV94vBN33oGFByGo1jyPXaGzBc7ZQ9sv8oy2K0IH4PobrXIIH1b8Ciiw+CEGC0905kEXgzGyKeAqu3PD3/q5KStLv2/6vNXN+F7O3vC3EOpCFdOz0AEzz9KIR8QO7iS6E5jFeDBCLnVCjNbMOsCWIJoMmPHzb3npzUv/rV4wo3hgy3l2NeyxerZ6f/Wn6Ihqi2sX79NhdYLCzNfBTkSw31XiJv7R3wmLetPNi49pj/wDcdSFDsBSQKKnrZiaLZA6gLc5uP2CtuXrsejvnVzMrYN9gBQTuGKuED+52n+G2mi+4JXLdeOypbWQqyCus2rGASpRw3YYAyKN4vN113vT6is9v5QtuQPOnVptXM/TQ52qscUZYo3W67sElCjRw9GS010oWYbYX+KjroDX4/igjAseCKx33BBv5H+VqOL34zlY1HiM7ZgkwjYkLLsTZsEAhPzWFv3LsEfnCh4RO1CpsY9py3acPDcuPqBbV2yXj6E54MkD/yrhCsY+uRl5SqVh69F4TCAOEIXlk20LKiwLWDmhw4XgRFFbzB25nWM61oKDNvFJ0toPqpuBeBX69VuDExaifHjHJW2LX2DJWZ90+B5waSum7M/utSugpjFPKcfunidZpo4BrHl9EgBg0SU9N8oVwkcM4++Cx/7AOeJOmy/9mKKrbqVG9tNlPAGOI7pgo6DeVfBdjdKDbRjsPc2Y1U14ogJLpzoreHgdrmuEGPrMqMW5ekvOdOHBYdOdfrT2Wi/OicPghLSWTXf68naEcBYc68jWc4kambnSyhXBgzGFriyTt7eiKk18tK3O4zGxy1E4OZdxGWMAOEiwlJZP26dt8Q/Kw7BjCpS3H8AuVCm96YIaIZXYIeHXFpsWHeFRBrNd1u6VkT1YGclQF4qDYV0I7TQ2TpyQIYNz/akZyTLGZoG0PukwkxlEfeXSiXvR6E4r/QmALWDi27NgAoM1cw6yJW8N1v6MGdf5tT11bP1OdOjYu+ms9sh1YI3oXXQDExzCEMF/4KoQl7FxiDW99jiwkgbNq8c9Hp1xB3F+InyUIlMcTkjG9YEyQ45zqudwzZeuk41qtSvAv18faCCOSLyGIRA2YijKXy4NZ2w4Au8JixEu9+Z7/fhz3cFjYgcKqwsMeVJGQoYJzlPGtlg37zBf5OXq5dC2mMML/igrzVoMANyTiydvfWksaGT1rIN8/lh3TDUCyxMTCNZWCUDMzp0TVi/LMYJ1cZHRbwZc3wiRQ/uzCCF4OEFWTN9HBWN0pr5N5nPZ2vcGSqSsg+oAgx/Sh1wF8TUzMFhdOXVHe+QaCBNkypdEexS4IDcwgRPlet8LecukpnlHO/IgYyuVxhpcgxBGVHvoIJEaRs3ckdv4sT6zrYcj0PoNOYOoTYcX46zQIW0pY57EtFRZjzAW3MWjYlepcS4/OUI6EBBUHpza7z/3C6MJ6kyh+ulzJdS2moOkSszIGk1fjByW5gP+01xwQJDoi3gZuqba4tjeq2zV6aOQDkYrlJihkmNQy0VO3f5svYQm99/wJVfQGqQfwGWwRb1ORXj1JaOg678RM94oqfsl5zCqmn4Qp8kgNeZbc/nMHQ7KG/cfRuMdyu1Ezp3ZvjK7DW61mAa2WMilZEaQu4ZaUnf4oMQufY7At+oABkljIvz3TtxE0bgi6ad4UTi+ZjwHzUBeJBbdRnEB4rLbl5/8khcKK3zz4qPUv9kCmtBnHcWIF5mmDNjAydbWImkLDJQwQGq3LUiD59fTtrqPR8UOKwlh+TTjZAEuAHS+XWqyfsRKZa1ADItVc64DSdm62bgywwgmScyC/1uWHGNxxElutDqsuXL6jrIa/D8PFxyF+BAViKkzNzQXxfoWyItCjbD1DekTGN1sUaFBTq43tXVBY1/BhcCMFGKd43uto5IJe1LnWjNsWrZfA6TNmIUIMIKfOniN5v/1j+m+MrthkgOz8xg4jfsPMRsE6o0z/85gyZf7StaW+nrBfggkvzMQwfk1dWdrzivlVcQc7GcchzhJolGs+FF5IgL14MggwKQaYmyoz0YzhbrtC7G3U7JmVo6NO2ojBiMFgjtw7i/UoItvK/eA4FGxA0gAfmtDjDbM978Yz4YFlqRglH04Q6na2TjobRx1kMQLETl14Jq2xcKa2Qe5PrJwlYyWDTZ4eMe8WShKUFDfu/5GHw6wOntbcb4bJ5IabwjkTt1pmS63xbgNzdh1xqyVrdEVAwjEG+kYsPgwi1w13UDqXncWu1BfG+T1mSXNogqkjDonNt/rb7qfbN1WXuxuuv9QNrbqimv10AD7ERdmTGW1fC3cEeTvha4TqtNvA8vy4OJoEuHVs7ccqkK1EI535cY+fL3r8XuEmGDhowAB4YlKTXxseiM4TngPMOdQe6dyKp3F42KHWVlMMxsvUszSIV6FVjI6N6885CXYkO/j7Iwn6lSRCgKXxBpc/Bg5FhkqNvZtPq8+O6hDyxFJx2Zih3hDNOWOIB0gepxIAb7FwL/aAiT2mLarjRL2rErAn3Hsw5boAXxvzEhjP2J9jiKxu9HxPQFf1tIVUAvNo4oBXPDsfiuX1Gx/uHrD/osVP4rpsXIEzkFbCd2Bg+vf8XsCs7QTNjenF8rCMksrA7g+UAKnX1+ps8Ynn+KWJHhMRiBOmkcTLMRuMZEJb8kYlwY4x2EdopIFNej415N4XOxwYqfOHp/NUCNwv5ZN9XVll02xpCrAWnOFKs186JXJNDx2ImZUdeDCYvIig09Cu7OwgHPZDPEmRm1CvPFb0G5oBZp7uL066JE5tonR0J7o6YKPY9CwwChuaf21QP2oLb4bC0d9jf9naysgODuLbwQr/a+/2YcnyXAdWJ9/uA/DBi2v0NbqtLohPo7YPFpYIXR1/eJ97uqCXNgT+/5lKw/laWjGYX0s8F74G2RcwI12FT1WzIOuDTwudqBaszzarKm2QQMJwPs3nf+ywyBGCDiXq29/FtZIoYoZOBhu/GEYSSAIW5ed4MfrUAur/i3uRJlSdGW9me0oWBAoF/tWYLk65BXN2NOW0uZMwAmViGXZcy3gTsJlaF5i3FebvLB3kiF1Rvi2RIgclsVFFwVX4NSxC90ovTIaOI6nLiq8V8bcSbhZxuzhW2lMl9U0tusa+qvrKl7DYnyvtcrwOEsTeq+lLrVm0syhW3nbsHaW566cvsvnNt4H1iHO645jKnP7NFfBNW4J+RDFiB3ZstGEQDkNURqEDzZaIdjRmN1cOeMAu7BoHPlTgij0UzzbX9AWOYumNCnatjQfQK0uQHNKjGglajgWO84RMr1eg3yJPXxLEqf+iWekkJyJVZ7wndAvDxa0cT8DiDTieT3qz9a2BC5RY6jBwsQ1wTGHGyR8W2AcwPiGEeIu6AH5S/tCPFEEdxPWGuLkSDmJGDUMRYgSmmfQM+dPSqmyxqPocSIqQyML5zWGiRCS06oQRsBAjfMW3aEBYnQYzEu76OHpWFutZteCTqCNublLmIsRUkA2LjjMnWtxQaInlTtUb5lHmdD+3UtMfyOPDS4y3h81tc6MZrETRTW1lvAejx881x59eyDKaEuNbPWu46txUBijmpl7hiDxPitLOjCJlTCK6aAAq+65ujCEb0uESKHYBDJLDXOF+p2KsOgh0+DY7st0UrmsJ/ZeVfev8A09+TCTDksOLeeRhrVt+XG/r8H9PVfY5UUq2Zp/e1LiNK5XKenA/cUEJUTPXkw20MSuQqNcprWs8NVR0Lxq5n5O9XDUbskWKTPFpeixI/qbebRMhASjSX3X8/sXrWp/FlYHxdHGSQ+AnfjoTuCL3abFR7lw3BUKV8pA8450pJb9y3AaijFOin2BldhcfV93SJExjungBpf6zvVvlxIjWLD06VMDtxNlhxDEDcoggUuKNWCQR2c9kKGkE7FkdMlBvhysRusb4sY4F5Ajh+sdr7N+HlkEsAxzKYMIpWbYpgOPD2I5ov1ymjJgIx1xIu786P5zPtdhrNgzbAJN7NAYQF8Axhp8GctFqAQm3U8UNaalfbY7IBb39rX/CwzWGCYkUKuLGU1nSJM9AXccMQZyEQe8cdn9ZSGdZZk6wGgY6g4V1cAy51AHjn3o2eo6OLGQQxjY4AQPpy4oo3WH/Xf730faI+FbETN+ZDY8MFlgDyRzl0zQk/r8Ol8ZJAfo9KEb1KX2TJ7hnzJwo/Yq4sRjxPESJI9umiLF16A69kbxwfkBEWygLMR+M+toW4ktxRYlxnGTVrSLQ8UVlg9AK6rSiXpx1YUtsH4IPgeCbo9AEzsAq8osQI4v9lb56WXrujYxYQRT49hJZm4arJzkmeLwxe4MWNwkfpJohMV0rIFootOGp7A1K4buyXeuPeJ8P3eAlTty5a/+qi8wsCCt5muARNS3BusOJ7wnxc4o5oJzxIwTmQ0PPdnXjMppBnAuLCyxVJnjqusX62TE4Nl9tCebPngztSrjt4Hq+M0t+HXIxzO7Dq1BGgq6nmC5gDrtC2lbibskQ9DOH7/Fn4PQDIidKBqNXd+MytTNxl2Rxvc2b+9149IDpSmW1lj2CFSxq9BQubJK1Iw7ARc8TtkiDhJ9HYFW1ymVoJmluaCfW5k6rgU8y9bPQa8NKS3ogPrwzjOPraFZPkVfqpN9qPbIF7Rzx7oLC8bt0La4DlZky5I3CZ9UOtj3xqaqgQUaD5iFLhA7RDmZJ0DpUfZQbbVHgrPA6sbAgxSSiya12a3LTmAXEuulJEwRnZcELVIlAy8RgBxTxPuQy4mOPaM6+e24/ceYytygFGWQtrIEcF1hRb6lZzrzIkrW1PMZzpqARaWQRIwZ33Q5EipjKDs3n81ZJAWd2H+NJvVZx0tQGtF/DxZOskegih1aa8dPGt3faIyLMX2OBGxNBRRkY+sZ1zq42CCAGHFcAaVasDqt0yhgjqNg2RO94bDe7NULd6lm2wLaFl8QoA0dNiQtmeD8mhpmpMkR30/s8cO7Tw7Xj7U1IGO0dAXUNiNh2ni8UT+5TUsHCgjoRAv3pnLj3NoWwRWSqHMMxxTLaFqDx7Cqek2uQbXbFaRuE3/mFc0gfPAW4Hoi/eqX9gUpddZ4tOCvHXwsrMFM6txDHTh0ZN1/EdcisgaSpovNHcsjKwPFGsTlMIFVqlZWnkxMlCqGem0s6je7NjfRxZoZKA9EMUEmNZDDpTaCFewARNoegSp2AKVC6CFmDdY6xUI9nqCksibw/tbWBIKmCZLH+GIOuwIO9osnfmesMAOKJfsCSpdaMyhR8p9MqzkiRwvHM8mf1f+GtF6ibXUd7sXPmUfq/zWXOYuDdSxChQ2uXqs9sMKZrhRGfh9e0V8MByfsmtkHtEfug3wtJIn/NrCMtkVwBQx6GMyP7/HbkGPR37s4cf3ssZuclD9v9D+8PYYauPo0mksXT93mWVg0ZYWwhAz9A23U0kaswTodSECGMKHMEm7z/dtPuXxsjGG9D53lU/dye6ilk3fzehI3rzxiTwR17khLAyhFzJQnEeUsmoJdVtx0cJrdvvaYPwvpLvYIdLFD52FcdPqFhy+FCxKdDzxFgfLoc+d7Yb59/c7tWV6sGI/lCK3jGkiGhvkOM99duv0ykw9835m2W3xj4SJ0ikBg2Mxcd4brFx/wRAt4o0ZYn+Ip2Tq1B8rh/CVoq7/BIsr2QNLy0HZLtUcWEHvMVii5GuF9BzjMiiNpdNMi5xdHMjJXXYA7156iNn+WZ/EUXAeDHgahs4eva1ssoF3YzcsPeUA6os5zrNin95+ElYdYLPLrxij3FVURmD09d8S85T9cZTTs9CmRipte9JtRh1r0K60965f3bz9yjBpxZZQTosSx/YiKFCNORL7WEP4A6+Yc4slInENxEkXjdBedC8dvciYCzldHTWIDXezg58O90cUD1gIU2pNUbJiTrUUcSNwQNyji4poH1kza1pJbVetpLRgNsRp/u4qT+LGr/N1jLS2fsY8adi7GtYO2SJExLn9m+MihqU35SXTlzF3tGefBwjkh1MmIQQUuRcfRlbRnbJM8fSx/VhxmUe/eeGyzVO7A1vO0Y90pDmQbGbKoPv8G68kprMUwWFmsrraYB1hBbkjrxVw5g6UlBfeA2CEBGMsPWHfVwVofGJAwMYGC/agxw9PQNkvYOgODF9bnfxEHx/HD9WB0Y430mlqTGnUtSgUq2G7I++LZa66JH7akAcfvp/+5iWdWsdZuk54l+DVYOPyJ+iw058W6vamzxCW0gtdBHili3VioHBkB9gh0sQOYqHilZW4jUGlrnQl3wdJ7YdAlWV3giBfFTfIjr3vgLpj4wGLKOCHevbXUo/4Q/AceqRoXGqO9yjH4u841p9PY3qupSuM8Nkc4neQZY7MLjtERB65BvpF216ww0rvRPEu8Tp2MyGzvrkblCFEcF76jcBvWq3XaCE5oWJnDTNYrPXv4BrWtMImyF0husxvyjD3tKISyArAPsR8gnsGDB+WVz+z18zMyqtMK+r3qZLYYh1qt8eouCJJ7GuwrM2xsdghyO82whCjcBw1g4TJy3zmrhhyRomLNC99jj2soYQqEgSypHDgfYUCgeid/uXQsUpgptQWu8Z8zDaa/e641PX90MPFx4fgt9izaj6zEa2LoM6oQZTCm80oeiBOljEkpM8ehOzeesMut88+qk1yV7cwSBF9F7PKWTs31dbBakAqSraD/FcQCSuk62dmSwOjjbG88e6A33+JTnbmm8OlDVCh8UlZqSC5iLvxTV5oxZAs9vGeeJoIyuL97rKG8kTtySczvf1agbn9X0561TYoMcfhEQ9813GAV92wwhxoXHkOH7awyhr5f7StP4Ux1mPPIKO8zvabTK3rBjUBVBvKfIEw6mEDCe/6qBB4VL0hL6FxzBtXKPoRP1L83NNNe6R9YwsvPdWXXAuVFmDCC+4N/q2UYSMPVRWArpQcxv7mjtvPyeJMHbqTqzfJyCoIZeD+4MSiDwg3nAG64QI1ARFB1gwFBfz3XGRsmVOyBwnf9b3EDGCSMgoeH+vvqr4WVbEy8xm/FOYVcTtywZCI+wxh6wOPnSmSwL/XXosgemQKuULBCev5ya+cc1LYQ5SiSQllslu+F4//4/kuq27EwPwYn91/lBraFYnbmNZGTpYtNWQuax4GxYHW5pH24uzCqI1Yqi6yJHQMhS/4kHA4pFKML1cs9ggY0X8jbj+64zGll1Vrk5TWXI0QKTbOGb+MKDH2dGuyny6dvc7pYwUrqdznAI+vGOkPfJvNok3JHStTMQh1HVda2eg4ELWtmGaJ+EdHi050pWgCSlY0gxwfT7bBSMFmBCwknLYQVOUjcU19d3MgKv3YeLsIbQogyrxptsAo+3AJneKdc/XyROlIcZZmiKzPyj1BSA3SxSKXMeJjzmKnCSYmUDrwGHWUwGmJBoV5TanwZlV1h+pDNvM4FhAyuMEQQ8G999Y5n0REuqNY0D3VyoWB7/+bz3IEYzUWx6BEGPAgOXCJcxLASYMli/yInDxb0J/pMabMm4F6CmEW0xcKxO6lD86kUnnx/b1B1EmDCxwyEJ95/8p2tfkavaNzs5k7VT4MkQRqpTwqhPsFXjMKFC80zzkZgkTx77hv3fE1vqV7TIn6K3dHhd2ivZRSWfF2wsOFCmTa3hIhbd/t5Q++pVPmsNHSp8xYvxOK30uM5f3QDuploeagYmCOpffb4/nOq36ko1bMSO3QzqeczgltsvVGiDa9p/tGO2rO+oCZ9aLtlvO+tuwxhAhHW5LTdbfxdl+iYUi/3cEsHciVamLldd633lwRhnPewENG6DaEpnDO9p1ni3qtn7qchbZdyOo29taB1vprYnVM/qmCmLrRhf2+7cauAUDFVf7Yel5/vpm3xLKj7g1t56sA1unn5wZel7TCyw3KNpg5YsnSxOI8IlRsQRldBs1Hj5ApKZnavP0NnlahdPX+PHqMQW1kJ4cKHZBcAYpClQFIqWycbhXFiMXJ7wErEWgEHt1+k58rqgMWC5gqoOYTlia6zttaHcARm29DpBq2nEBRH2yqAkxyzrNh/cJ+QYoB9gEHEEfeUgCJtAh2R3QGxZLQxwgyiM+xcc1oJhP/KAGeAtY7FafAbdbAsJMonnU1+twb7DfsI+ZWuUFpZzC+fvqaWA8pw9Q1AbOzXYqOpWfdS1LRXSd4GMACVStSLwkcMxTP2D+4+o7mH2nO2gzU96s+hLYuPshBi30BWoCy6hQrRgpChrtYYs4eru2zyHh5k8XfIs2uivkON3/LRoR0XKXOeJNStzkzaofb9tgcDtL8iqpn1T47xFSiflrr8XV3bapuvJnaCIHwfILdt5rAt3AZs7uEO2lbiUAWew/KIWInv6M5L1PHn6dyGDWKFuB3y7vCcNXVyDKVbVx99WYZUFzpeVEdZ8LrgIaYON7x22wLUuIdlAkJnRIflvPQBFtlGk1+4/cjgaDO4HM3/awcPwvDY9EHhytl7VDfXMP6sCVtacm22I0TsBMHLeHTvBVVI0ZeFYtSqJtzcVufezSfcqQRxRIQdIFQIw8AaazWonJ9YKKzSRvlH833rxaogUphBRdpJh2pT/HTmxmciQTmjTyIavryRttXC+WM3aULvdXT26A122TEhA3e2avM8VNUwC9+j3mzave4ML/Y9/9gf2lb7iNgJgheCBHe08EciMJrDmqFPpiBEY2TbihPU45dZFD5yGD8uOCbyMHHQZkh5rqqYPXwbTR20kROSdQsPkoO0qIhRw9H0PW1MU0bw2Yj7mU00wR0um6Q3z/C3+bOcwyVYdb7KbKwgCN8XjXsU5/y6iydu09FdV7StfoHImQnd2G6rqWvtmRQxWjiD0H3mDALkVOqNOHMUTc6ubJiwIfh5AAsQ4oeZcMzcnj7od5EsgM81EzowuOViTizH5zgrdEDEThC8kHhJo3OBPfIrURLmLGgYsHDcTp7Q0S01HUz2IJ639GxXbYulw/b2RwN5Usu6SgfWHQQN4ojUKiyh6QyorNi24jjPdEOwXUHEThC8lK5/V+cySzS/XDjWfrcdrNVcRrmOyKOLECUMW2eYsNCTxjmnUBluO9ee9pOcDo7suMw1tVhildNnnqNZgGXFPLiqSOLHBEWvBnO0v7BNu0qT2aJDFgKaB7iCiJ0geCko6fulQ0G+P6TdMhYuM9BGvWD0TjzxgPiaLnSw7jCriokM1LGmzhafbt14SOeO+a2bPbLzEpd4Ne9bmktHq7XIw+ktelI23g+C98/qU1Qr6xDeZsas4Vvp7rXHPMExcG5dbavzyASFIHg5NTL/yTmPSOxFm3RrZo/YRn91WcU5kNZuK2QDSfTDljZkAdOZPXIbRYoS1k+H8Dkjt3MZWKGKvlUONZWoocrCOuYHJUJ5JmJ803e34YXiddD5pFqGwZy8XaBCeuoyzvVVyETsBMHLufPvY6qUpj8H/bEAVtuhFXg7lgnA+hNIKNfTSqxB7h3cUFeT+CGeiyfu5rQS4/tCjT59/MgJ+6iUKKg1EsBEBqonUEW08lJ33uYqwXoqtPuCIHgh6GKMXDgU1V84fptbnGF9V7QZQ3zMTOgAhAl/i+7C4Vyo3EELKbiwiPMZ3xsPkV+HiYu1cw5xWd7cUdu4Fhhu9tRdbfg7uYPE7ARB4Ca7KLBHuySsKnbzygOur7YldAC102jCgHpbV8hWKJlWO2vbqYTLjITheWO285KMEMeOIytTnES+rq2reNyN3bH6lGm7Fb3xpbUfbg0yt1ETZ1bnd+bQNYqbNLrN0QNT2uhrZatP3q51p7mNkRlYZjBtjgSm+UQosEffLusDjnhCqqxxKcqP/msp8Vr8DV6PnCKY6Vii0Zlmk+gPh3wnBGrL1s3G5rotkJ905vANypIvibbFF7SPR5E9vgPiITi8GEGxj9ABFj39rcH6nYlSxODkUCOol0RrJWOKgadAEThqZTGKo0NLvKQ/as845sqZO3yxGWs0bXFQ7d/MBZJZle9bwPFEW3LjfrEFjhPOUZQ1OQLNEyAEmJ3E+YCAfopM/vv/2QMX+fG9Vym7EoivQV2f4dyMAcfEntDp4HdZOsG8pa7jq9ttmrtr3Rka2XE5Pbr7gsKEC8FJwY7A+YvYHrodt+gfsA7VHrfsKpXur93zy4pp+7l7iC1QYF0iQw8WPCODWy2miydvaY/88+b1B+pYdar2yD/TBm6mHWtOaY/80rbiZFOhA70bzaWF43bR0sl7aMkkyw31e7eumK+WdXjHZerbeD6tmL6Plk/Zy/WHRWN1pWYlxmmvMAetbWYP385Cd0edaD4ROrDQ2AImfV8buVFrZh2ixRN2c2ud/k0X0JjOq7jmcdH43bRDuSlGcKI2yD9Ke+TL8in7eK2BwBI6tPXpUnsG53uhALxRgdE0bdAm7VnHtCw1nurmGq49ckyrspOoZz3/S1U+e/iC+jWZrz2yD5YhrFdwBI3ssELbYp/b1x7R75Wn0P6tF2jvxrM0sd8GyhqiDadZOMuU/huoduEhdOlU4C+HCaYpNzFStPCcIuIMOD/QvTh3iVR+JiDM2Lr0OHcbRocgZ4QOoEtNbmU8BVTogMfFLiKZ+9OwbrBTbIFRunI1H6qXZ4S2xRe0UbKetTGCHW6vpVHTPiVpvtZX35pVM/ZT6Tq2c3Uw1T50SQOe5h40z3IbtaqxTSsAry9XPzuvhzmQX1+Ptj8eRJ8/fqbpf27WXuWX9fMOccnOiBWNeGnI5v1K0z+PBlKzYmNtnnBwAVB0bUb3idW5s0TfmbUpf9m0VLtdAeo3qw43vmzau5T2Kl/gumCtAVhY1gxqudBf7aKn+Kvbaj6mi0925v1foWFOWn+jN01VYgdrzxHHlbVUpFpG7pWHTjTOEDdxVM4RMy4YjosOy/c5w4zBm2nyxlacS+YssORaDSxL7YZV5GNw4N1w7nfYpeYM7RX22bL0GE1c/htN7GO+jGBgMP9oB+50Y1zIyhZwfc+o44YStJEdV9Cpg9eoctoBtEcJPHrQrZ93WA2++7gHXow4kfg6cYYnD15QnjJpqO+M2tqWgOFxsXMXJDcWqZyRG3uif5UnwYLdWDTEGCJAN4VabfJrj8yBm+MKehNEa8rXz2HzokRPOiw4bA1ciN0vhgR4rQVM4+u5TPYYtKCen4TOXg3mUmc3pvadZYmyMrFsnhF0Vj5ltb6ALaYO3kSVG+Wias3zsEA6A2otZ+1r51KnaSNr5x5itz5T3sS0fv5hbat99Nb+1mAghOXuaHX+fZvOUVb1efnKplHW4Xlt69dh6ZkuPGmBkIkj4O7C4MBggnUsWpT4m9uDoSfjiA7L6M/Wi2mEEkEsJMWNSR1YdXBd0ZS0ZK2s1HtqTW1rwPluxA6gaWOnv6pwPzVPryKfQ5002w1u3P1bz/wJjTV63MsVQpj0Vdu+8iRlVIJrRkVlzU3qv5FXWbLGnV547oJE0ZptCyr3bDnHl/ZvOacsLufX3EU8EGkBzoD4WKyE5vu8UKX0VKmxj/bINmjljYaf+ZTlume9bXffGjQ2xeDx+4iKbq0lcnDbBcqrrAzwc8t8NGfENr7vLsVrZKZNi+0vQIRu2DV/y8f3M+VOTBsX+l/RKzDB0gQplWWK0JKj0D6uFb38C7FqNOtkq1l5ILhhQR3860jocC7dV65uw67FqMNIx+unuMJ3JXZ6ofCIFb9SkyJj+b6n+LlVfpph5UrChS3nYC2MiFHDUPlkfbgdTrlkfal4HPv5PZhUOLDtIlsbE5TbgRgULAn08KqpPt8MnCDbHvTndjUl4vfg9urOuHKe5tduxXgwqJ5xME3c2lLb6hxISs0dwbcvmj1uX3tid4BxBNaNtS7+hqu+dq5vi3FHVG+Rly4pK//gVtfWAZ45bCuvmwqQRHvn+hMOnLsLesk9uP1Ue2QOmp1iPRWAJT5nqe/wtUHYBhY0ko5RNeEIiB7Ws4gaMyIFD2kZ+J2Z6ICYcmfnx6/Z6sdqZp7G42JnT/+d89QtJ1O6nAloQm/PubMwqbGU4TvNApkzajvVMlms2hqsDzDncAdei2LxqU60wmEy42dudJgoZQxKniEObVAjMVY1/3tjc+15c2BZDZpfj9b+24sqN/Gh7nVnUftKU7Rnvx4t+5XhLHp003UFvD6WjVl2I5iFs16m0lWwiDiy8/WBEftr3ij/8Vh7TN/TllqVm8D3nQ2UozMHZn717164cnqeuHIXrJtgL0yxdNIeDn8AJO/imsD6qKha+No07V2SY9doyf7mtd81ms1A/B3Lma6ctt9unF4H74f1JsJGDEWLTnTiyY7AwONip4xX7Z5f0K7l00dn5Y6oz/RaNHvE9i9Z054gf9l0tGXpcb6Pflp6n3tb4Pui5g8XBNxZW7O2OkjxiJPE4l7lUy7PkEUNaEyXVdqzzoGY0IJjf9CbN+8I68d+TcJFCkURori+dgVKhrC4jjMgveT8CfOZdQjJVWXN2ALdatEw8o/q05Q1OVhZoYN4/6JVvaOl/axB2kjj7iWoU43pynoPC9NDe8YcxOfgnsGCrZ19KFu/J/dfUxezZW1Vdzix9zIlS2+7uy7alKPfHH5jrWxDuLwKoQ14JN+CHIVT0NprvSihEny4tcZif2uQFIy1MqYO3Ki+s/3JHwg51o5Fnh/Oe2da8buLx8UuesRIbBEZOX3wX0qUKqb2yDnGrG1CdbIPozDhncv5cUSVpj6chrF1+XFlETi3/qhuQTgLd3/QiJUgCqXOGpdWTLV9UWBiAAtbG8E6mugU8TWBuDs7U+YucN9g4l+7cE/b4suSiXt4ptYWKARv2rMEx5LmHenIt6k7W1NVdSwx2eQKWCXr0snbtGHBYdPFcqzBTPrMve1ozqH22ud24NrNYGoAvKjew1VgHe1cc4ZylzS3YOAeP3n4iqbsaPXld6JmFY0uJ/XfoL3q64MBf5zyUjqNrcKWGBKKbVl5EDx7QofrCr8TBgR+W+tB5bRnAg+Pix1OolZlLC6CDhIrd2849yXAawZ2mnHHoTcW1oRdNfvAF//fDPyVvZFGB/EPzE6O/mMlVWnmWOzwfULY+Vwj+B7G39Bv9i/Us4H//C6dCg1yUtX0A/2JzMQ+66msGu3MwEc4YyXzVzF8H0e4+HK36DezNlsqb61SG5CTNqrjck6ZsQVmQ0tpTSGtqdIsDy0ev1t7ZI6ZiE/a/ht1rjXT7kWJ1baw/CLqQ41gmb+ZQ8xTigDOBeNar5iVLxG3B03c9pu2xT+YsGrQ2Xd1Lx2s74pSqQs2LOOvRfHqmWnLvf48IGMNYGdcWx1OQlYiidladEFZcqYLxU8eXXs2cPG42DXqWoxSZIpDeSJ1pBpZ/uS1P/s1mU8rL9h3c3Aymp2QHUZVoqg/RqAP7+yImdrRzi4diMTHSNEsSwU6AgtM+4Rvz8vM5VW/B7dsIdtQXxtJqJwpbxAhuMG12xWiYe2Walv8ki5XQp6QKZu0D1VOPYDdFqyS1nF0ZUpgtQqVNZjmd2YVMT1z31kwE+bMoBFQMuROpFzfRlQ+ZT9eTBk5WVjeb9Gpzry/zDi66zLFTRTNNM4VPXZEJQKhudLAFmbBdQhH++EV6Lm28r0Z0/7cQnW1iQkjFRrkoDVzDmmP/INzDGu7Imm8lnJ/q6lBDUs/YjHoDFadQoxsVNYm1kE2o0arfDRz6Bbt0bcFC1uv/bcnZc6bhB4oVxR5obZED+cWRO7Fk9dqEM9OWx8M4Hjr1yRQu54gVwZlSI5iXd4AYhOO0ljs9fz/r6IH/B3FZSFWttp06zjzGldBWMJSx2kbXEKeCLPoOHq/wPidAQWi/lfX1bR30zkWeSywjokKDJ4o4Efcu1zd7JxSgvjnt0BaPAmC4DEweYEyyTWzDvKEEvrYIXWlSpPcLH7fEhE7QRA8DmQFsfoMPom0Ld8eETtBELyC78vxFwRBCCRE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8ApE7ARB8AKI/geC2hVb/ZzoVwAAAABJRU5ErkJggg==)###############

###############################################################
###################..........Vahid Ashkanichenarlogh______#################
###################..........Student Num:251260738______#################
###################..........Londan, ON, Canada___________#################
###############################################################

# ***Some useful links that helped me through this research.***


*   https://github.com/shenyichen105/Deep-Reinforcement-Learning-in-Stock-Trading
*   https://github.com/ucaiado/QLearning_Trading
*   https://github.com/Albert-Z-Guo/Deep-Reinforcement-Stock-Trading
*   https://github.com/ThirstyScholar/trading-bitcoin-with-reinforcement-learning
*   https://github.com/AI4Finance-Foundation/FinRL-Meta
*   https://github.com/Kostis-S-Z/trading-rl
*   https://github.com/Rahul-Choudhary-3614/Deep-Reinforcement-Learning-Notebooks
*   https://github.com/lazyprogrammer/machine_learning_examples/tree/master/rl

# **Connect to the Google-Drive to access to the data and required storage**
"""

from google.colab import drive
drive.mount('/content/drive')

#############################################################
#############################################################
#      Vital point: I put it here to be sure about it. Thanks Prof.

#      Please in order to use the dataset, Save the 
#      training model, and load the trained model 
#      for test phase, change these address by your
#      own google drive address. read the attached csv
#      file of your google drive for csv_address (below).
#      Also create new folders in your google drive, then 
#      change the "models_folder" and "rewards_folder" for
#      these folders. 
#############################################################
#############################################################
#Please note that in order to simplifying the problem we have a dataset to  use "close price" in the csv format.
#Rows in our data correspond for separate day, and columns are for different stocks.
csv_address = '/content/drive/MyDrive/FinalProjectRL/aapl_msi_sbux.csv'
models_folder = '/content/drive/MyDrive/FinalProjectRL/rl_trader_models'
rewards_folder = '/content/drive/MyDrive/FinalProjectRL/rl_trader_rewards'

"""# **Import the required libraries and load the data**"""

import numpy as np
import pandas as pd
from tensorflow.keras.models import Model
from tensorflow.keras.models import load_model
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.optimizers import Adam
from keras.models import Sequential
from keras.layers import Dense, Activation, Flatten, CuDNNLSTM,LSTM
from datetime import datetime
import itertools
import argparse
import re
import os
import pickle
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

#############################################################
#############################################################
#  Let's use AAPL (Apple), MSI (Motorola), SBUX (Starbucks)
#  returns a T x 3 list of stock prices
#  each row is a different stock
#  0 = AAPL
#  1 = MSI
#  2 = SBUX
#############################################################
#############################################################
get_data1 = pd.read_csv(csv_address)
get_data=get_data1.values
plt.plot(get_data[:,0], color='orange')
plt.plot(get_data[:,1], color='green')
plt.plot(get_data[:,2], color='black')
plt.title("Changing trend of the three markets")
plt.legend(['AAPL', 'MSI', 'SBUX'])
plt.show()

### The experience replay memory ###
class ReplayBuffer:
  def __init__(self, obs_dim, act_dim, size):
    self.obs1_buf = np.zeros([size, obs_dim], dtype=np.float32)
    self.obs2_buf = np.zeros([size, obs_dim], dtype=np.float32)
    self.acts_buf = np.zeros(size, dtype=np.uint8)
    self.rews_buf = np.zeros(size, dtype=np.float32)
    self.done_buf = np.zeros(size, dtype=np.uint8)
    self.ptr, self.size, self.max_size = 0, 0, size

  def store(self, obs, act, rew, next_obs, done):
    self.obs1_buf[self.ptr] = obs
    self.obs2_buf[self.ptr] = next_obs
    self.acts_buf[self.ptr] = act
    self.rews_buf[self.ptr] = rew
    self.done_buf[self.ptr] = done
    self.ptr = (self.ptr+1) % self.max_size
    self.size = min(self.size+1, self.max_size)

  def sample_batch(self, batch_size=32):
    idxs = np.random.randint(0, self.size, size=batch_size)
    return dict(s=self.obs1_buf[idxs],
                s2=self.obs2_buf[idxs],
                a=self.acts_buf[idxs],
                r=self.rews_buf[idxs],
                d=self.done_buf[idxs])

"""# **Normalization step by Scaler**"""

################################################################################
################################################################################
#########  this function is to normalizing the states. the idea is that ########
#########  to get the right parameters of our scaler, we must have some ########
#########  data. In order to get this data, I am running a random episode#######
#########  to see which state encounter. there is no need to have an agent.#####
#########  So, you can see that to perform an action we just need a sample######
#########  of action space When we are done, Standard Scaler is fitted to ######
#########                  the state we encountered.                  ##########
################################################################################
################################################################################
def get_scaler(env):
  # return scikit-learn scaler object to scale the states
  # Note: you could also populate the replay buffer here

  states = []
  for _ in range(env.n_step):
    action = np.random.choice(env.action_space)
    state, reward, done, info, _trade = env.step(action)
    states.append(state)
    if done:
      break
  scaler = StandardScaler()
  scaler.fit(states)
  return scaler

# To create a new directory if it doesn't exists before, to save our model
# and weights. I will use this function later.
def maybe_make_dir(directory):
  if not os.path.exists(directory):
    os.makedirs(directory)

"""# **MLP: Multi-Layer Perceptron Model definition**"""

###############################################################
#0#############################################################
#                                                             #
#             """ A multi-layer perceptron """                #
#     In this part I defined a model by MLP to play as        #
#     an agent with one hidden layer. the number of of        #
#     outputs is the number of action we have(I talked        #
#     about the actions and the number of actions at          #
#     next cells.) Our loss function is Mean Square Error     #
#     and Adam is used as our optimizer. Please note that     #
#     we can change our hyper-parameters (Number of hidden,   #
#     loss-function, and our optimizer) to tuning our model   #
#                      in other ways.                         #
#                                                             #
#     ****I am even going to use LSTM in this work****        #
#     ****and compare the results for both models ****        #
#     ****to prepare it at-least for a conference paper.      #
###############################################################
###############################################################
def mlp(input_dim, n_action, n_hidden_layers=1, hidden_dim=32):
  # input layer
  i = Input(shape=(input_dim,))
  x = i
  # hidden layers
  for _ in range(n_hidden_layers):
    x = Dense(hidden_dim, activation='relu')(x)
  # final layer
  x = Dense(n_action)(x)
  # make the model
  model = Model(i, x)
  model.compile(loss='mse', optimizer='adam')
  print((model.summary()))
  return model


'''def LSTM(input_dim, nb_actions, nb_cell_1,nb_cell_2, nb_unit):
  input_dim = Input(shape=(input_dim,))
  model = Sequential()
  model.add(LSTM(nb_cell_1, input_shape=input_dim))
  model.add(Dense(nb_unit))
  model.add(Activation('relu'))
  x = model.add(Dense(nb_actions, activation='linear'))
  model = Model(input_dim, x)
  model.compile(loss='mse', optimizer='adam')
  print((model.summary()))
  return model'''

"""# **Multi-stock Environment definition**"""

################################################################################
################################################################################
#             Here I am going to define the environment class.                 #
############       A 3-stock trading environment.             ##################
############       State: vector of size 7 (n_stock * 2 + 1)  ##################
############       shares of stock 1 owned                    ##################
############       shares of stock 2 owned                    ##################
############       shares of stock 3 owned                    ##################
############       price of stock 1 (using daily close price) ##################
############       price of stock 2                           ##################
############       price of stock 3                           ##################
############       cash owned (can be used to purchase more stocks) ############                         
############       Action: categorical variable with 27 (3^3) possibilities ####
############       for each stock, you can:                   ##################
############       0 = sell                                   ##################
############       1 = hold                                   ##################
############       2 = buy                                    ##################
################################################################################
################################################################################
  
class MultiStockEnv:
  #please note that the initial_investment is $20000 as cash. we used it later.
  def __init__(self, data, initial_investment=20000):
    # data
    # set our input data to an attribute
    self.stock_price_history = data
    #here we are defined 2 attributes "n_step or days & n_stock", that I get from our data shape
    self.n_step, self.n_stock = self.stock_price_history.shape

    # instance attributes
    self.initial_investment = initial_investment
    self.cur_step = None
    self.stock_owned = None
    self.stock_price = None
    self.cash_in_hand = None
    # The action space is 27. because we have 3 actions to do (sell, buy, hold), and 3 stocks.
    self.action_space = np.arange(3**self.n_stock)

    # action permutations
    # returns a nested list with elements like:
    # [0,0,0] #it means sell all stocks
    # [0,0,1] #it means sell AAPL and MIS stocks but do nothing (Hold action) for third stock.
    # [0,0,2] #it means sell two first stocks and buy SBUX in third stock.
    # [0,1,0] #it means sell first and third stocks but hold the second (MSI) stock.
    # [0,1,1] 
    # etc.
    # 0 = sell
    # 1 = hold
    # 2 = buy
    #by below code we put actions by [0,1,2] format in a list.
    self.action_list = list(map(list, itertools.product([0, 1, 2], repeat=self.n_stock)))

    # calculate size of state
    self.state_dim = self.n_stock * 2 + 1
    #it cause to return to initialized state.
    self.reset()


  def reset(self):
    self.cur_step = 0
    self.stock_owned = np.zeros(self.n_stock)
    #it says the stock price in current day.
    self.stock_price = self.stock_price_history[self.cur_step]
    # the amount of cash the we have
    self.cash_in_hand = self.initial_investment
    #we return the state vector which is done by a function that we will see the inside
    # of this function shortly.
    return self._get_obs()


  # This function perform an action in the environment and return reward and next-state, and some other things.
  def step(self, action):
    # first we checked in that action we passed in, exist in our action space.
    assert action in self.action_space
    # get current value of our portfolio before performing the action
    prev_val = self._get_val()

    # update price, i.e. go to the next day (cur_step is as day),(stock_pricegoes to next day)
    self.cur_step += 1
    self.stock_price = self.stock_price_history[self.cur_step]

    # perform the trade
    _trade = self._trade(action)

    # get the new value after taking the action
    cur_val = self._get_val()

    # Here I am computing the reward which achieved by current portfolio value - previouse portfoio value
    #reward is the increase in porfolio value
    reward = cur_val - prev_val

    # done if we have run out of data
    done = self.cur_step == self.n_step - 1

    # the info flag store the current value of the portfolio 
    info = {'cur_val': cur_val}

    # conform to the Gym API
    return self._get_obs(), reward, done, info, _trade


################################################################################
################################################################################
#                 This description is for _get_obs function.                   #   
#                 get_obs means getting the next state.                        #
#                 this function return the state.                              #
#        sometimes like now, we like to use term state in observation          #
#        interchangeably. there are some case, such as some transformation     #
#        observation. or even be multiple past observations stacked up         #
#        together. as mentioned earlier, this function is going              #
#        to have three components. The fisrt component is the number           #
#        of shares in each stock owned. this should be a list of three.        #
#        The second component is the value of a stock. this also should        #
#        be a list of 3. Finally I added chash money to the last index         #
#                   in our list, and then returned the observation.            #
################################################################################
################################################################################
  def _get_obs(self):
    obs = np.empty(self.state_dim)
    #first component
    obs[:self.n_stock] = self.stock_owned
    #second component. 
    obs[self.n_stock:2*self.n_stock] = self.stock_price
    #third component
    obs[-1] = self.cash_in_hand
    return obs
  

################################################################################
################################################################################
#        This description is for _get_val function.                            #
#        this function return the current value of our portfolio.              #
#        Let see how will I calculated the value of the portfolio?             #
#        Lets consider we have 10 shares of AAPL, 5 shares of MSI,             #
#        and 3 shares of SBUX. If share prices be $50 for AAPL, $20            #
#        for MSI, and $30 for SBUX, and we will have $100 cash, then:          #
#                 Value = 10*50+5*20+3*30+100 = $790                           #
################################################################################
################################################################################                           
  def _get_val(self):
    return self.stock_owned.dot(self.stock_price) + self.cash_in_hand


  def _trade(self, action):
    # index the action we want to perform
    # 0 = sell
    # 1 = hold
    # 2 = buy
    # e.g. [2,1,0] means:
    # buy first stock
    # hold second stock
    # sell third stock
    action_vector = []
    action_vec = self.action_list[action]
    action_vector.append(action_vec)
    #print(action_vec)
    # determine which stocks to buy or sell
    sell_index = [] # stores index of stocks we want to sell
    buy_index = [] # stores index of stocks we want to buy
    for i, a in enumerate(action_vec):
      if a == 0:
        sell_index.append(i)
      elif a == 2:
        buy_index.append(i)
    #This part is considered to simplifying the problem.
    # sell any stocks we want to sell
    # then buy any stocks we want to buy
    if sell_index:
      # NOTE: to simplify the problem, when we sell, we will sell ALL shares of that stock
      for i in sell_index:
        self.cash_in_hand += self.stock_price[i] * self.stock_owned[i]
        self.stock_owned[i] = 0
    if buy_index:
      # NOTE: when buying, we will loop through each stock we want to buy,
      # and buy one share at a time until we run out of cash
      can_buy = True
      while can_buy:
        for i in buy_index:
          if self.cash_in_hand > self.stock_price[i]:
            self.stock_owned[i] += 1 # buy one share
            self.cash_in_hand -= self.stock_price[i]
          else:
            can_buy = False
    return action_vector

"""# **Define agent properties, load and save functions for model**"""

################################################################################
################################################################################
#             In this part I am going to describe the agent.                   #
#       The agent is AI, that must learn to maximize the feature rewards.      # 
################################################################################
################################################################################       
class DQNAgent(object):
  ############################################################################
  #          first we have constructor. that correspond to                   #
  #          state size and action size. these are correspond                #
  #          to the number of input and output of our neural network.        #
  #          Next we have some hyper-parameters that you can see.            #
  ############################################################################
  def __init__(self, state_size, action_size):
    self.state_size = state_size
    self.action_size = action_size
    self.memory = ReplayBuffer(state_size, action_size, size=500)
    self.gamma = 0.95  # discount rate
    self.epsilon = 1.0  # exploration rate
    self.epsilon_min = 0.01
    self.epsilon_decay = 0.995
    self.model = mlp(state_size, action_size)

  def update_replay_memory(self, state, action, reward, next_state, done):
    self.memory.store(state, action, reward, next_state, done)

  #this function takes a state and uses epsilon greedy to choose an action based on that state
  def act(self, state):
    #first we generate a random number between 0 and 1, and check the list of epsilon.
    #if it is, then we perform a random action. Otherwise, we perform a greedy action
    #by grabing the all Q-values for the input state. Then the action is that has maximum value.
    if np.random.rand() <= self.epsilon:
      return np.random.choice(self.action_size)
    act_values = self.model.predict(state)
    # the output of our model is batchsize by the number of outputs. so, I index the return value
    # by 0, before taking the argmax.
    return np.argmax(act_values[0])  # returns action



  def replay(self, batch_size=32):
    # first check if replay buffer contains enough data
    if self.memory.size < batch_size:
      return

    # sample a batch of data from the replay memory
    minibatch = self.memory.sample_batch(batch_size)
    states = minibatch['s']
    actions = minibatch['a']
    rewards = minibatch['r']
    next_states = minibatch['s2']
    done = minibatch['d']

    # Calculate the tentative target: Q(s',a)
    target = rewards + (1 - done) * self.gamma * np.amax(self.model.predict(next_states), axis=1)

    ###########################################################################
    ###########################################################################
    #        With the Keras API, the target (usually) must have the same      #
    #        shape as the predictions. However, we only need to update        #
    #        the network for the actions which were actually taken.           #
    #        We can accomplish this by setting the target to be equal to      #
    #        the prediction for all values. Then, only change the targets     #
    #        for the actions taken. Q(s,a).                                   #
    ###########################################################################
    ###########################################################################
    target_full = self.model.predict(states)
    target_full[np.arange(batch_size), actions] = target

    # Run one training step
    self.model.train_on_batch(states, target_full)

    if self.epsilon > self.epsilon_min:
      self.epsilon *= self.epsilon_decay

  #function for load the saved model
  def load(self, name):
    self.model.load_weights(name)
  #function to save the model
  def save(self, name):
    self.model.save_weights(name)

"""# **Play the agent for one episode**"""

def play_one_episode(agent, env, is_train):
  # note: after transforming states are already 1xD
  state = env.reset()
  state = scaler.transform([state])
  done = False
  trade_in_each_action = []
  #num_episodes = 0
  while not done:
    #num_episodes+=1
    action = agent.act(state)
    #actions.append(action)
    next_state, reward, done, info, _trade = env.step(action)
    #if num_episodes==4:
    trade_in_each_action.append(_trade)
    #rewards.append(reward)
    next_state = scaler.transform([next_state])
    if is_train == 'train':
      agent.update_replay_memory(state, action, reward, next_state, done)
      agent.replay(batch_size)
    state = next_state

  return info['cur_val'], action, reward, _trade, trade_in_each_action

"""# **Adjusting some parameters as well as plotting the data distribution**

"""

# Here I have defined some parameters 
num_episodes = 500
batch_size = 32
initial_investment = 20000
#check for directory. it is exists or not.
maybe_make_dir(models_folder)  
maybe_make_dir(rewards_folder)
#get data and declared to n_timesteps, n_stocks
data = get_data
n_timesteps, n_stocks = data.shape

n_train = n_timesteps // 2
#split train and test data
train_data = data[:n_train]
test_data = data[n_train:]
#By all below codes, I showed the distribution of each stocks in train and test sets. 
fig, ((ax1, ax2)) = plt.subplots(1, 2, figsize = (20, 7))
ax1.plot(train_data[:,0], color="orange")
ax1.plot(train_data[:,1], color="green")
ax1.plot(train_data[:,2], color="black")
ax1.set_title("--------------------- Train Data ------------------------", fontsize = 14)
ax1.legend(['AAPL', 'MSI', 'SBUX'])

ax2.plot(test_data[:,0], color="orange")
ax2.plot(test_data[:,1], color="green")
ax2.plot(test_data[:,2], color="black")
ax2.set_title("--------------------- Test Data ------------------------", fontsize = 14)
ax2.legend(['AAPL', 'MSI', 'SBUX'])

"""# **Training the model (agent)**"""

env = MultiStockEnv(train_data, initial_investment)
state_size = env.state_dim
#action_size = len(env.action_space)
action_size = len(env.action_list)
agent = DQNAgent(state_size, action_size)
scaler = get_scaler(env)

actions_list = []
rewards_list = []
# store the final value of the portfolio (end of episode)
portfolio_value = []
actions_trade = []
trade_action_in_each_ite = []
avg_rawards = []

args = 'train'
# play the game num_episodes times
for e in range(num_episodes):
  t0 = datetime.now()
  val = play_one_episode(agent, env, args)
  dt = datetime.now() - t0
  print(f"episode: {e + 1}/{num_episodes}, episode_end_value: {val[0]:.2f}, duration: {dt}")
  portfolio_value.append(val[0]) # append episode end portfolio value
  actions_list.append(val[1])
  rewards_list.append(val[2])
  actions_trade.append(val[3])
  if num_episodes==499:
    trade_action_in_each_ite.append(val[4])

# save the weights when we are done
if args == 'train':
  # save the DQN
  agent.save(f'{models_folder}/dqn.h5')

  # save the scaler
  with open(f'{models_folder}/scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)
  # save portfolio value for each episode
  #np.save(f'{rewards_folder}/{args.mode}.npy', portfolio_value)

  #if num_episodes >= 2:
          #avg_rawards.append(np.mean(portfolio_value[-2:]))

plt.plot(portfolio_value, color="Blue")
plt.title("Train_phase")
plt.xlabel("Number_of_episode")
plt.ylabel("episode_end_value")
plt.show()

#Here I am printing some useful values that I needed for evaluation purpose.
print("############################################################################################################################\n")
print("These outputs are the output of model prediction at the end of each episode\n Actions indexes in action list of below:\n\n ", actions_list)
print("############################################################################################################################\n")
action_list = (env.action_list)
print("\nList of possible actions for 3 stock.\n 0 = sell\n 1 = hold\n 2 = buy \n\n", action_list)
print("############################################################################################################################\n")
print("\nReward at the end of each episode resulted by the model: \n\n", rewards_list)
print("############################################################################################################################\n")
print("\nActions list in 0,1,2 format at the end of each episode resulted by the model\n\n", actions_trade)
print("############################################################################################################################\n")
print("\nWhole actions in 0,1,2 format at each iteration, resulted by the model\n\n", trade_action_in_each_ite)

"""# **Plot the actions trend for each stock through the different episodes**



"""

#Plot the actions for each stocks through the different episodes.
AAPL = []
MSI = []
SBUX = []
for i in (actions_trade):
  for j in i:
    #print(j[0])
    AAPL.append(j[0])
    MSI.append(j[1])
    SBUX.append(j[2])
fig, ((ax1, ax2, ax3)) = plt.subplots(1, 3, figsize = (20, 7))
ax1.plot(AAPL, color="orange",)
ax1.set_title("Train phase: Actins changing on AAPL data\n through the different episodes ", fontsize = 14)
ax1.legend(['AAPL'])

ax2.plot(MSI, color="green")
ax2.set_title("Train phase: Actins changing on MSI data\n through the different episodes", fontsize = 14)
ax2.legend(['MSI'])

ax3.plot(SBUX, color="black")
ax3.set_title("Train phase: Actins changing on SBUX data\n through the different episodes", fontsize = 14)
ax3.legend(['SBUX'])

"""# **Re-training the model for next 500 episodes**"""

#Please note that because most of these codes are repeated, So I avoid to commenting them more.
#Reloading and retraining the model for the next 500 episodes
env = MultiStockEnv(train_data, initial_investment)
state_size = env.state_dim
action_size = len(env.action_space)
agent = DQNAgent(state_size, action_size)

#some lists for different values 
portfolio_new_value = []
actions_new_list = []
rewards_new_list = []
actions_trade_new = []
with open(f'{models_folder}/scaler.pkl', 'rb') as f:
  scaler = pickle.load(f)
  # remake the env with test data
  env = MultiStockEnv(train_data, initial_investment)

  # make sure epsilon is not 1!
  # no need to run multiple episodes if epsilon = 0, it's deterministic
  agent.epsilon = 0.01
  num_episodes_train = 500
  args = 'train'
  # load trained weights
  agent.load(f'{models_folder}/dqn.h5')
  for e in range(num_episodes_train):
    # play the game num_episodes times
    val_new = play_one_episode(agent, env, args)
    print(f"episode: {e + 1}/{num_episodes_train}, episode end value: {val_new[0]:.2f}")
    portfolio_new_value.append(val_new[0]) # append episode end portfolio value
    actions_new_list.append(val_new[1])
    rewards_new_list.append(val_new[2])
    actions_trade_new.append(val_new[3])
    # save the DQN for 1000 episodes
    agent.save(f'{models_folder}/dqn_1000.h5')

    # save the scaler for 1000 episodes
    with open(f'{models_folder}/scaler_1000.pkl', 'wb') as f:
      pickle.dump(scaler, f)

plt.plot(portfolio_new_value, color="Black")
plt.title("Performance of the trained model on test data")
plt.xlabel("Number_of_episode")
plt.ylabel("episode_end_value")
plt.show()

"""# **Test the model on test data**"""

#Test phase
#Please note that because most of these codes are repeated, So I avoid to commenting them more.
env = MultiStockEnv(train_data, initial_investment)
state_size = env.state_dim
action_size = len(env.action_space)
agent = DQNAgent(state_size, action_size)
def play_one_episode_in_test_phase(agent, env):
  # note: after transforming states are already 1xD
  state = env.reset()
  state = scaler.transform([state])
  done = False

  while not done:
    action = agent.act(state)
    next_state, reward, done, info, _trade = env.step(action)
    next_state = scaler.transform([next_state])
    agent.update_replay_memory(state, action, reward, next_state, done)
    agent.replay(batch_size)
    state = next_state
  return info['cur_val'], action, reward, _trade

portfolio_test_value = []
actions_test_list = []
rewards_test_list = []
actions_trade_test = []
with open(f'{models_folder}/scaler_1000.pkl', 'rb') as f:
  scaler = pickle.load(f)

  # remake the env with test data
  env = MultiStockEnv(test_data, initial_investment)

  # make sure epsilon is not 1!
  # no need to run multiple episodes if epsilon = 0, it's deterministic
  agent.epsilon = 0.01
  num_episodes_test = 600
  # load trained weights
  agent.load(f'{models_folder}/dqn_1000.h5')
  for e in range(num_episodes_test):
    # play the game num_episodes times
    val_test = play_one_episode_in_test_phase(agent, env)
    print(f"episode: {e + 1}/{num_episodes_test}, episode end value: {val_test[0]:.2f}")
    portfolio_test_value.append(val_test[0]) # append episode end portfolio value
    actions_test_list.append(val_test[1])
    rewards_test_list.append(val_test[2])
    actions_trade_test.append(val_test[3])

#Point: I don't know why I faced with error at the end of 600 episodes to plot. But there is no problem. 
#Because It is taking 11 hours to run again just to plot. So, I plot it here correctly 
plt.plot(portfolio_test_value, color="Black")
plt.title("Performance of the trained model on test data")
plt.xlabel("Number_of_episode")
plt.ylabel("episode_end_value")
plt.show()

#printing some useful values for us to evaluating our model in test phase.
print("############################################################################################################################\n")
print("These outputs are the output of model prediction at the end of each episode\n Actions indexes in action list of below:\n\n ", actions_test_list)
print("############################################################################################################################\n")
action_list = (env.action_list)
print("\nList of possible actions for 3 stock.\n 0 = sell\n 1 = hold\n 2 = buy \n\n", action_list)
print("############################################################################################################################\n")
print("\nReward at the end of each episode resulted by the model: \n\n", rewards_test_list)
print("############################################################################################################################\n")
print("\nActions list in 0,1,2 format at the end of each episode resulted by the model\n\n", actions_trade_test)
print("############################################################################################################################\n")

#Plot the actions for each stacks through the different episodes in test phase.
AAPL = []
MSI = []
SBUX = []
#These list are just including sell and buy actions, not including hold action. just to have good and sensible plot for output.
AAPL2 = []
MSI2 = []
SBUX2 = []
for i in (actions_trade_test):
  for j in i:
    #print(j[0])
    AAPL.append(j[0])
    MSI.append(j[1])
    SBUX.append(j[2])
for k1 in MSI:
  if k1!=0:
    AAPL2.append(k1)
for k2 in AAPL:
  if k2!=0:
    MSI2.append(k2)
for k3 in SBUX:
  if k3!=0:
    SBUX2.append(k3)
fig, ((ax1, ax2, ax3)) = plt.subplots(1, 3, figsize = (20, 10))
ax1.plot(AAPL, color="orange")
ax1.set_title("Test_phase: Actins changing on AAPL data\n through the different episodes ", fontsize = 14)
ax1.legend(['AAPL'])

ax2.plot(MSI, color="green")
ax2.set_title("Test_phase: Actins changing on MSI data\n through the different episodes", fontsize = 14)
ax2.legend(['MSI'])

ax3.plot(SBUX, color="black")
ax3.set_title("Test_phase: Actins changing on SBUX data\n through the different episodes", fontsize = 14)
ax3.legend(['SBUX'])

#I converted List to array to use in our plot at the below cell
    print(AAPL)
    actions_over_time1 = np.array(AAPL)
    actions_over_time2 = np.array(MSI)
    actions_over_time3 = np.array(SBUX)
    print(actions_over_time1)

#Plot the test data (separately for each stock) along side the action in each step of our data to increase the portfolio value.
fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(15, 20))
ax1.plot(test_data[:,0], color="blue", ls = 'dotted')
ax1.plot(train_data[:,1], color="green", ls = 'dotted')
ax1.plot(train_data[:,2], color="red", ls = 'dotted')
ax1.set_title("--------------------- Test Data ------------------------", fontsize = 14)
ax1.legend(['AAPL', 'MSI', 'SBUX'])


ax2.plot(actions_over_time1.flatten(), label = 'Actions')
ax2_copy = ax2.twinx()
ax2_copy.plot(test_data[:,0], label = 'RSI', color = 'blue', ls = 'dotted')
ax2.legend(['AAPL'])


ax3.plot(actions_over_time2.flatten(), label = 'Actions')
ax3_copy = ax3.twinx()
ax3_copy.plot(test_data[:,1], label = 'RSI', color = 'green', ls = 'dotted')
ax3.legend(['MSI'])

ax4.plot(actions_over_time3.flatten(), label = 'Actions')
ax4_copy = ax4.twinx()
ax4_copy.plot(test_data[:,2], label = 'RSI', color = 'red', ls = 'dotted')
ax4.legend(['SBUX'])
plt.show()